{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Аникеев Федор М33031 lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Zpq4QOU5Wg-H",
        "i_7DyyXRWg-K",
        "_JewKs4XU-so",
        "5yiLk1P_xYQ2",
        "VlWxW3e9Wg-m",
        "D39SSh0zWg-r",
        "rhVrgkSaWg_K",
        "XsRf9T_SWg_U",
        "ylKZG2MwWg_f",
        "9hedBdcYWhAH",
        "JrqW55jgWhAR",
        "5QYTwyMtWhAZ",
        "DbJrUpARWhAd",
        "MI18l-l9WhAk",
        "1wrEGqBSWhAr",
        "gStgBJy2WhAx"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX9p5jfTySS"
      },
      "source": [
        "## Задание 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "Применим полученные навыки и решим задачу анализа тональности отзывов. \n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать параметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpwmEZ58V9M0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcWQBhN7VItG",
        "outputId": "b767b2fe-a89c-40dc-840b-514e42aaee5e"
      },
      "source": [
        "!wget https://www.dropbox.com/s/maw1n18jz7x8jq9/sms_spam.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-25 20:29:42--  https://www.dropbox.com/s/maw1n18jz7x8jq9/sms_spam.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/maw1n18jz7x8jq9/sms_spam.csv [following]\n",
            "--2021-12-25 20:29:43--  https://www.dropbox.com/s/raw/maw1n18jz7x8jq9/sms_spam.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucaba9ea14e61428b79f58200c6e.dl.dropboxusercontent.com/cd/0/inline/BchqBY9CdG1lbrB1rkCrp6GB5h0nfn2VCXisM9ERDgID07NFfkJ9kIubK3qc3L7TVXYI_hVmM7DGZggIfEJy3yredWa6gmC7Nd7NlcjyzjDFs4pGI-Q9zrHBP8Y2QEU2VASij5ygiONLZ1FpEkdQyVyY/file# [following]\n",
            "--2021-12-25 20:29:43--  https://ucaba9ea14e61428b79f58200c6e.dl.dropboxusercontent.com/cd/0/inline/BchqBY9CdG1lbrB1rkCrp6GB5h0nfn2VCXisM9ERDgID07NFfkJ9kIubK3qc3L7TVXYI_hVmM7DGZggIfEJy3yredWa6gmC7Nd7NlcjyzjDFs4pGI-Q9zrHBP8Y2QEU2VASij5ygiONLZ1FpEkdQyVyY/file\n",
            "Resolving ucaba9ea14e61428b79f58200c6e.dl.dropboxusercontent.com (ucaba9ea14e61428b79f58200c6e.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucaba9ea14e61428b79f58200c6e.dl.dropboxusercontent.com (ucaba9ea14e61428b79f58200c6e.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 480877 (470K) [text/plain]\n",
            "Saving to: ‘sms_spam.csv’\n",
            "\n",
            "sms_spam.csv        100%[===================>] 469.61K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-12-25 20:29:43 (5.11 MB/s) - ‘sms_spam.csv’ saved [480877/480877]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bKHjjkCV950",
        "outputId": "d068c291-870d-4656-b4b2-ad8a7e8116dc"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 6.1 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VuoVT6cWHRJ"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLSBQ8acZXmV",
        "outputId": "40c407f6-7383-4149-f4da-2b752585c61a"
      },
      "source": [
        "data = pd.read_csv(\"sms_spam.csv\")\n",
        "print(data.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5559 entries, 0 to 5558\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   type    5559 non-null   object\n",
            " 1   text    5559 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.0+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_9rmxgbVzGn"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqF46eBeWWM3",
        "outputId": "ddcdadac-cff8-4740-e92f-0579c723b8a8"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHerMC0LWX6W"
      },
      "source": [
        "noise = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc71LkgKXL63",
        "outputId": "9037740b-9950-46a9-a230-a052b756b7a6"
      },
      "source": [
        "print(data.iloc[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type                                                  ham\n",
            "text    Hope you are having a good week. Just checking in\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQO3QP2QZ7rk"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz4bYaR7Waj8"
      },
      "source": [
        "for i in range(data.shape[0]):\n",
        "  data.iloc[i][1] = data.iloc[i][1].lower()\n",
        "  data.iloc[i][1] = data.iloc[i][1].split()\n",
        "  for j in range(len(data.iloc[i][1])):\n",
        "    pol = pymorphy2_analyzer.parse(data.iloc[i][1][j])\n",
        "    data.iloc[i][1][j] = pol[0].normal_form\n",
        "  data.iloc[i][1] = \" \".join(data.iloc[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9EOMAezaiRI"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data.text, data.type, train_size = 0.7)\n",
        "def test_vectorize(vectorizer):\n",
        "  # x_train, x_test, y_train, y_test = train_test_split(data.text, data.type, train_size = 0.7)\n",
        "  smart_vectorized_x_train = vectorizer.fit_transform(x_train)\n",
        "  \n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(smart_vectorized_x_train, y_train)\n",
        "\n",
        "  smart_vectorized_x_test = vectorizer.transform(x_test)\n",
        "\n",
        "  y_pred = clf.predict(smart_vectorized_x_test)\n",
        "  return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), recall_score(y_test, y_pred, average='macro')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKBMhvfgtdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f10fd8d-758f-4ed0-f424-635434782d62"
      },
      "source": [
        "output_table = pd.DataFrame(columns=['Vectorizer', 'accuracy', 'f1_score', 'precision', 'recall'])\n",
        "count_table = pd.DataFrame(columns=['Vectorizer', 'accuracy', 'f1_score', 'precision', 'recall'])\n",
        "ngrams = [(1,1), (2,2), (3,3), (4,4), (1,2), (1,3), (1,4), (2, 3), (2,4)]\n",
        "for i in range(len(ngrams)):\n",
        "  count_vectorizer = CountVectorizer(ngram_range=ngrams[i], stop_words=noise)\n",
        "  acc, f1, prec, rec = test_vectorize(count_vectorizer)\n",
        "  # print(\"CountVectorizer: \", \"ngrams range: \", str(ngrams[i]), acc, f1, prec, rec)\n",
        "  output_table.loc[output_table.shape[0]] = [\"CountVectorizer: \" + \"Ngrams range: \" + str(ngrams[i])] + [acc] + [f1] + [prec] + [rec]\n",
        "  count_table.loc[output_table.shape[0]] = [\"CountVectorizer: \" + \"Ngrams range: \" + str(ngrams[i])] + [acc] + [f1] + [prec] + [rec]\n",
        "print(\"Count Vectorizer results:\")\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
        "    print(count_table)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer results:\n",
            "                              Vectorizer  accuracy  f1_score  precision    recall\n",
            "1  CountVectorizer: Ngrams range: (1, 1)  0.985012  0.984909   0.984882  0.960691\n",
            "2  CountVectorizer: Ngrams range: (2, 2)  0.972422  0.971304   0.972568  0.903595\n",
            "3  CountVectorizer: Ngrams range: (3, 3)  0.942446  0.935578   0.945476  0.784722\n",
            "4  CountVectorizer: Ngrams range: (4, 4)  0.937650  0.929139   0.941831  0.764706\n",
            "5  CountVectorizer: Ngrams range: (1, 2)  0.985012  0.984818   0.984889  0.954940\n",
            "6  CountVectorizer: Ngrams range: (1, 3)  0.985612  0.985411   0.985518  0.955286\n",
            "7  CountVectorizer: Ngrams range: (1, 4)  0.985612  0.985411   0.985518  0.955286\n",
            "8  CountVectorizer: Ngrams range: (2, 3)  0.973621  0.972551   0.973877  0.906203\n",
            "9  CountVectorizer: Ngrams range: (2, 4)  0.973621  0.972551   0.973877  0.906203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAbDucpBqFnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4418fa-8f9a-462b-9187-f2a0193e090e"
      },
      "source": [
        "max_features = [None]\n",
        "df_boundaries = [(1, 1), (1, 100)]\n",
        "tfid_table = pd.DataFrame(columns=['Vectorizer', 'Vectorizer_params', 'accuracy', 'f1_score', 'precision', 'recall'])\n",
        "for i in range(len(ngrams)):\n",
        "  for j in range(len(df_boundaries)):\n",
        "    min_df = df_boundaries[j][0]\n",
        "    max_df = df_boundaries[j][1]\n",
        "    for f in range(len(max_features)):\n",
        "      max_feature = max_features[f]\n",
        "      count_vectorizer = TfidfVectorizer(ngram_range=ngrams[i], stop_words=noise, min_df=min_df, max_df=max_df, max_features=max_feature)\n",
        "      acc, f1, prec, rec = test_vectorize(count_vectorizer)\n",
        "      output_table.loc[output_table.shape[0]] = [\"TfidfVectorizer: \"  \"Ngrams range: \" + str(ngrams[i]) + \", min_df: \" + str(min_df) + \", max_df: \" + str(max_df)] + [acc] + [f1] + [prec] + [rec]\n",
        "      tfid_table.loc[output_table.shape[0]] = [\"TfidfVectorizer: \" + \"Ngrams range: \" + str(ngrams[i])] + [\"min_df: \" + str(min_df) + \", max_df: \" + str(max_df)] + [acc] + [f1] + [prec] + [rec]\n",
        "print(\"Tfid vectorizer results:\")\n",
        "# pd.set_option('display.expand_frame_repr', False)\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
        "    print(tfid_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tfid vectorizer results:\n",
            "                               Vectorizer       Vectorizer_params  accuracy  f1_score  precision    recall\n",
            "10  TfidfVectorizer: Ngrams range: (1, 1)    min_df: 1, max_df: 1  0.867506  0.929053   0.867506  0.500000\n",
            "11  TfidfVectorizer: Ngrams range: (1, 1)  min_df: 1, max_df: 100  0.964029  0.961569   0.965461  0.864253\n",
            "12  TfidfVectorizer: Ngrams range: (2, 2)    min_df: 1, max_df: 1  0.872302  0.817291   0.888687  0.518100\n",
            "13  TfidfVectorizer: Ngrams range: (2, 2)  min_df: 1, max_df: 100  0.940048  0.932284   0.943923  0.773756\n",
            "14  TfidfVectorizer: Ngrams range: (3, 3)    min_df: 1, max_df: 1  0.869305  0.810285   0.886417  0.506787\n",
            "15  TfidfVectorizer: Ngrams range: (3, 3)  min_df: 1, max_df: 100  0.909472  0.888230   0.918027  0.658371\n",
            "16  TfidfVectorizer: Ngrams range: (4, 4)    min_df: 1, max_df: 1  0.867506  0.929053   0.867506  0.500000\n",
            "17  TfidfVectorizer: Ngrams range: (4, 4)  min_df: 1, max_df: 100  0.898082  0.869059   0.908797  0.615385\n",
            "18  TfidfVectorizer: Ngrams range: (1, 2)    min_df: 1, max_df: 1  0.872302  0.817291   0.888687  0.518100\n",
            "19  TfidfVectorizer: Ngrams range: (1, 2)  min_df: 1, max_df: 100  0.955036  0.951009   0.957252  0.830317\n",
            "20  TfidfVectorizer: Ngrams range: (1, 3)    min_df: 1, max_df: 1  0.874700  0.822723   0.890514  0.527149\n",
            "21  TfidfVectorizer: Ngrams range: (1, 3)  min_df: 1, max_df: 100  0.949041  0.943701   0.951868  0.807692\n",
            "22  TfidfVectorizer: Ngrams range: (1, 4)    min_df: 1, max_df: 1  0.875899  0.825384   0.891431  0.531674\n",
            "23  TfidfVectorizer: Ngrams range: (1, 4)  min_df: 1, max_df: 100  0.944245  0.937684   0.947612  0.789593\n",
            "24  TfidfVectorizer: Ngrams range: (2, 3)    min_df: 1, max_df: 1  0.874101  0.821379   0.890057  0.524887\n",
            "25  TfidfVectorizer: Ngrams range: (2, 3)  min_df: 1, max_df: 100  0.929257  0.917760   0.934591  0.733032\n",
            "26  TfidfVectorizer: Ngrams range: (2, 4)    min_df: 1, max_df: 1  0.874101  0.821379   0.890057  0.524887\n",
            "27  TfidfVectorizer: Ngrams range: (2, 4)  min_df: 1, max_df: 100  0.923261  0.909246   0.929498  0.710407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnCE3mxZtaGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58c57e7-853d-4806-a2be-b256423eb9e5"
      },
      "source": [
        "count_char = pd.DataFrame(columns=['Vectorizer', 'accuracy', 'f1_score', 'precision', 'recall'])\n",
        "for i in range(len(ngrams)):\n",
        "  count_vectorizer = CountVectorizer(ngram_range=ngrams[i], stop_words=noise, analyzer='char')\n",
        "  acc, f1, prec, rec = test_vectorize(count_vectorizer)\n",
        "  count_char.loc[output_table.shape[0]] = [\"CountVectorizer(char): \" + \"Ngrams range: \" + str(ngrams[i])] + [acc] + [f1] + [prec] + [rec]\n",
        "  output_table.loc[output_table.shape[0]] = [\"CountVectorizer(char): \" + \"Ngrams range: \" + str(ngrams[i])] + [acc] + [f1] + [prec] + [rec]\n",
        "print(\"Count vectorizer(char) results:\")\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
        "    print(count_char)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count vectorizer(char) results:\n",
            "                                     Vectorizer  accuracy  f1_score  precision    recall\n",
            "27  CountVectorizer(char): Ngrams range: (1, 1)  0.970024  0.969480   0.969424  0.917548\n",
            "28  CountVectorizer(char): Ngrams range: (2, 2)  0.983813  0.983570   0.983680  0.950415\n",
            "29  CountVectorizer(char): Ngrams range: (3, 3)  0.983213  0.982944   0.983077  0.948153\n",
            "30  CountVectorizer(char): Ngrams range: (4, 4)  0.983813  0.983604   0.983657  0.952332\n",
            "31  CountVectorizer(char): Ngrams range: (1, 2)  0.981415  0.981175   0.981195  0.947116\n",
            "32  CountVectorizer(char): Ngrams range: (1, 3)  0.983213  0.982979   0.983050  0.950070\n",
            "33  CountVectorizer(char): Ngrams range: (1, 4)  0.983213  0.982944   0.983077  0.948153\n",
            "34  CountVectorizer(char): Ngrams range: (2, 3)  0.983213  0.982944   0.983077  0.948153\n",
            "35  CountVectorizer(char): Ngrams range: (2, 4)  0.985012  0.984818   0.984889  0.954940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3C-qQm5qY0z"
      },
      "source": [
        "# print(output_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Задание 5.2 Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4339e5-7945-497a-a394-3f9f2bbc72c8"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ec92a9-50be-48df-e814-11aadc09c33c"
      },
      "source": [
        "result = re.findall(r'\\b\\w{2}', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit,\\n sed do eiusmod tempor incididunt ut labore et\\n dolore magna aliqua.\\n')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lo', 'ip', 'do', 'si', 'am', 'co', 'ad', 'el', 'se', 'do', 'ei', 'te', 'in', 'ut', 'la', 'et', 'do', 'ma', 'al']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea9610e-cb36-4524-8f25-e2c36c210ef2"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d136e288-650c-443e-cddb-3d94355298f6"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e709e4fd-ab4e-4e90-9e80-cd317b75e957"
      },
      "source": [
        "text = 'предложение бла бла бла. sentence 2. TOday is a good day. Вечер добрый. До завтра.'\n",
        "result = re.split('\\.', text, maxsplit=2)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['предложение бла бла бла', ' sentence 2', ' TOday is a good day. Вечер добрый. До завтра.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0900fbb6-2bec-4d52-9f26-039c5446941c"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbcbbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721b8007-c699-4cfe-880d-0336f95ebe58"
      },
      "source": [
        "result = re.sub('[0-9]', 'DIG', 'asdknasodjn112ejndasjndo12ndo12jnd1o2jnd')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asdknasodjnDIGDIGDIGejndasjndoDIGDIGndoDIGDIGjndDIGoDIGjnd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be083ef4-546c-415d-b574-6135b6c25a93"
      },
      "source": [
        "result = re.sub('(http|https)://[^ \"]+', '', 'https://docs.google.com/spreadsheets/d/1qo03lRyhl5xM4_Kx3jzJSb9K2vRGm9dXIqbUDmrkxMc/edit#gid=57874460 asdasdhttps://piazza.com/class/kt725hi4odv52p asdasldm1212')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " asdasd asdasldm1212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047383b1-aa6f-4408-d822-5c797735db2d"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba9eebd-020b-48b7-8bb9-85facb32bf3f"
      },
      "source": [
        "prog = re.compile('[А-Яа-яё\\-]{4,}')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'больше', 'больше', 'слов', 'Что-то']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kp5eW200w8I",
        "outputId": "9f72af80-4a23-4070-b1ee-bfa25ef12fa7"
      },
      "source": [
        "text = 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'\n",
        "prog = re.compile('@[\\w.]+')\n",
        "prog.findall(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}